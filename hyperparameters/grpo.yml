# =============================================================================
# GRPO (Group Relative Policy Optimization) Hyperparameters Reference
# =============================================================================
# 
# GRPO is a reinforcement learning algorithm that uses group-based relative advantage
# estimation instead of a learned value function (critic). It generates multiple action
# samples per state and normalizes rewards within each group to compute advantages.
# 
# Key features:
# - No critic network needed - reduces computational overhead
# - Group-based advantage normalization for stable training
# - KL divergence regularization to prevent policy collapse
# - Compatible with continuous and discrete action spaces
#
# Paper reference: DeepSeek-Math (https://arxiv.org/abs/2402.03300)
#
# =============================================================================
# COMPLETE PARAMETER REFERENCE WITH TYPICAL RANGES
# =============================================================================
#
# CORE TRAINING PARAMETERS
# ------------------------
# learning_rate: Learning rate for the optimizer
#   - Type: float or Schedule
#   - Default: 3e-4
#   - Typical range: 1e-5 to 1e-3
#   - Notes: Can be a function of progress remaining (from 1 to 0)
#
# n_steps: Number of steps to run per environment per update
#   - Type: int
#   - Default: 2048
#   - Typical range: 128 to 8192
#   - Notes: Larger values = more stable but slower updates
#            Buffer size = n_steps * n_envs
#
# batch_size: Minibatch size for each gradient update
#   - Type: int
#   - Default: 64
#   - Typical range: 32 to 512
#   - Notes: Should divide evenly into n_steps * n_envs
#
# n_epochs: Number of epochs when optimizing the surrogate loss
#   - Type: int
#   - Default: 10
#   - Typical range: 3 to 30
#   - Notes: More epochs = more gradient steps per rollout
#
# DISCOUNT AND ADVANTAGE PARAMETERS
# ---------------------------------
# gamma: Discount factor for future rewards
#   - Type: float
#   - Default: 0.99
#   - Typical range: 0.9 to 0.999
#   - Notes: Higher values weight future rewards more heavily
#
# gae_lambda: Factor for Generalized Advantage Estimation (GAE)
#   - Type: float
#   - Default: 0.95
#   - Typical range: 0.9 to 1.0
#   - Notes: Trade-off between bias and variance
#            1.0 = high variance, low bias (Monte Carlo)
#            0.0 = low variance, high bias (TD)
#
# normalize_advantage: Whether to normalize advantages
#   - Type: bool
#   - Default: True
#   - Notes: Highly recommended for stable training
#
# GRPO-SPECIFIC PARAMETERS
# ------------------------
# group_size: Number of samples per state for group-based advantage estimation
#   - Type: int
#   - Default: 4
#   - Typical range: 2 to 16
#   - Notes: Higher values = more stable gradients but more computation
#            Core GRPO innovation - normalizes advantages within groups
#
# kl_coef: Coefficient for KL divergence penalty in loss
#   - Type: float
#   - Default: 0.1
#   - Typical range: 0.01 to 0.5
#   - Notes: Controls how much policy can deviate from reference
#            Higher = more conservative updates
#            Lower = more aggressive exploration
#
# CLIPPING PARAMETERS (PPO-style)
# -------------------------------
# clip_range: Clipping parameter for policy objective
#   - Type: float or Schedule
#   - Default: 0.2
#   - Typical range: 0.1 to 0.4
#   - Notes: Limits policy update magnitude per step
#            Can be a function of progress remaining
#
# clip_range_vf: Clipping parameter for value function
#   - Type: float, Schedule, or None
#   - Default: None (no clipping)
#   - Typical range: None, or 0.1 to 10.0 if used
#   - Notes: Only relevant when vf_coef > 0
#            Depends on reward scaling
#
# LOSS COEFFICIENTS
# -----------------
# ent_coef: Entropy coefficient for exploration bonus
#   - Type: float
#   - Default: 0.0
#   - Typical range: 0.0 to 0.1
#   - Notes: Higher values encourage more exploration
#            Can help prevent premature convergence
#
# vf_coef: Value function coefficient for the loss
#   - Type: float
#   - Default: 0.0 (pure GRPO - no value function)
#   - Typical range: 0.0 to 1.0
#   - Notes: Set to 0 for pure GRPO
#            Set > 0 for hybrid GRPO-PPO approach
#
# OPTIMIZATION PARAMETERS
# -----------------------
# max_grad_norm: Maximum value for gradient clipping
#   - Type: float
#   - Default: 0.5
#   - Typical range: 0.3 to 1.0
#   - Notes: Prevents exploding gradients
#
# target_kl: KL divergence threshold for early stopping
#   - Type: float or None
#   - Default: None (no early stopping)
#   - Typical range: 0.01 to 0.05 if used
#   - Notes: Stops epoch early if KL > 1.5 * target_kl
#            Helps prevent large policy updates
#
# EXPLORATION PARAMETERS (gSDE)
# -----------------------------
# use_sde: Use generalized State Dependent Exploration
#   - Type: bool
#   - Default: False
#   - Notes: Alternative to action noise exploration
#            Often better for continuous control
#
# sde_sample_freq: Frequency of noise matrix resampling
#   - Type: int
#   - Default: -1 (only at rollout start)
#   - Typical range: -1, or 4 to 64
#   - Notes: Lower values = more exploration diversity
#            Only used when use_sde=True
#
# POLICY NETWORK PARAMETERS
# -------------------------
# policy_kwargs: Additional arguments for policy network
#   - Type: dict or None
#   - Default: None
#   - Common options:
#     - net_arch: Network architecture, e.g., dict(pi=[256, 256], vf=[256, 256])
#     - log_std_init: Initial log standard deviation, typical range: -3 to 0
#     - activation_fn: Activation function (th.nn.Tanh, th.nn.ReLU, etc.)
#     - ortho_init: Orthogonal initialization (default: True)
#
# LOGGING AND MISC PARAMETERS
# ---------------------------
# tensorboard_log: Directory for TensorBoard logs
#   - Type: str or None
#   - Default: None
#
# verbose: Verbosity level
#   - Type: int
#   - Default: 0
#   - Values: 0 (none), 1 (info), 2 (debug)
#
# seed: Random seed for reproducibility
#   - Type: int or None
#   - Default: None
#
# device: Device for computation
#   - Type: str or torch.device
#   - Default: "auto"
#   - Values: "auto", "cpu", "cuda", "cuda:0", etc.
#
# stats_window_size: Window size for rollout logging
#   - Type: int
#   - Default: 100
#   - Notes: Number of episodes to average for reported metrics
#
# rollout_buffer_class: Custom rollout buffer class
#   - Type: class or None
#   - Default: None (uses RolloutBuffer)
#
# rollout_buffer_kwargs: Arguments for rollout buffer
#   - Type: dict or None
#   - Default: None
#
# =============================================================================
# ENVIRONMENT-SPECIFIC CONFIGURATIONS
# =============================================================================
# Below are tuned configurations for specific environments.
# Use these as starting points and adjust based on your results.
# =============================================================================

## Custom envs
#donkey-generated-track-v0:
donkey-mountain-track-v0:
  # Normalize observations for stable training
  # TODO: try normalizing reward too
  normalize: "{'norm_obs': True, 'norm_reward': False}"
  env_wrapper:
    #- utils.wrappers.AutoencoderCorrectionWrapper
    - utils.wrappers.AutoencoderWrapper
    - utils.wrappers.SpeedWrapper
    #- utils.wrappers.ContinuityCostWrapper:
    #    weight_continuity: 3
    - gymnasium.wrappers.TimeLimit:
        max_episode_steps: 10000
    - utils.wrappers.HistoryWrapper:
        horizon: 2
  callback:
    - utils.callbacks.PLNTensorboardCallback
    - utils.callbacks.LapTimeCallback
    - utils.callbacks.WandbCallback:
        verbose: 2
        model_save_path: "/tmp/stable-baselines/"
        model_save_freq: 100
        gradient_save_freq: 0
  # vec_env_wrapper:
  #   - utils.wrappers.VecForceResetWrapper
  n_timesteps: !!float 2e6
  #policy: 'CnnPolicy'
  policy: 'MlpPolicy'
  learning_rate: !!float 3e-4
  # Number of steps to collect per environment per update
  n_steps: 2048
  # Minibatch size for optimization
  batch_size: 64
  # Number of epochs when optimizing the surrogate loss
  n_epochs: 10
  # Discount factor
  gamma: 0.99
  # Factor for trade-off of bias vs variance for GAE
  gae_lambda: 0.95
  # GRPO-specific: Number of samples per state for group-based advantage estimation
  # Higher values provide more stable gradients but increase computation
  group_size: 4
  # GRPO-specific: Coefficient for KL divergence penalty
  # Controls how much the policy can deviate from the reference policy
  kl_coef: 0.1
  # Clipping parameter for the policy objective (PPO-style)
  clip_range: 0.2
  # Whether to normalize advantages within groups
  normalize_advantage: True
  # Entropy coefficient for exploration
  ent_coef: 0.0
  # Value function coefficient (set to 0 for pure GRPO, >0 for hybrid approach)
  vf_coef: 0.0
  # Maximum value for gradient clipping
  max_grad_norm: 0.5
  # Use generalized State Dependent Exploration (gSDE) for continuous actions
  use_sde: True
  # Sample a new noise matrix every n steps when using gSDE
  sde_sample_freq: 16
  # Limit the KL divergence between updates for early stopping
  target_kl: 0.01
  policy_kwargs: "dict(log_std_init=-2, net_arch=dict(pi=[256, 256], vf=[256, 256]))"

# Alternative configuration for faster lap times with more aggressive exploration
donkey-mountain-track-v0-aggressive:
  normalize: "{'norm_obs': True, 'norm_reward': False}"
  env_wrapper:
    - utils.wrappers.AutoencoderWrapper
    - utils.wrappers.SpeedWrapper
    - gymnasium.wrappers.TimeLimit:
        max_episode_steps: 10000
    - utils.wrappers.HistoryWrapper:
        horizon: 2
  callback:
    - utils.callbacks.PLNTensorboardCallback
    - utils.callbacks.LapTimeCallback
    - utils.callbacks.WandbCallback:
        verbose: 2
        model_save_path: "/tmp/stable-baselines/"
        model_save_freq: 100
        gradient_save_freq: 0
  n_timesteps: !!float 3e6
  policy: 'MlpPolicy'
  learning_rate: !!float 5e-4
  n_steps: 4096
  batch_size: 128
  n_epochs: 15
  gamma: 0.995
  gae_lambda: 0.98
  # Larger group size for more stable advantage estimation
  group_size: 8
  # Lower KL coefficient for more aggressive policy updates
  kl_coef: 0.05
  clip_range: 0.3
  normalize_advantage: True
  # Small entropy bonus for exploration
  ent_coef: 0.01
  vf_coef: 0.0
  max_grad_norm: 0.5
  use_sde: True
  sde_sample_freq: 8
  target_kl: 0.02
  # Note: vf network architecture is kept for potential hybrid GRPO approach (when vf_coef > 0)
  policy_kwargs: "dict(log_std_init=-1, net_arch=dict(pi=[512, 256, 128], vf=[512, 256, 128]))"
