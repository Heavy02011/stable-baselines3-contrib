# GRPO Hyperparameters for Autonomous Racing (Donkey Car Simulator)
# Group Relative Policy Optimization configuration optimized for autonomous racing tasks.
# GRPO uses group-based advantage normalization and KL divergence regularization
# for stable training without requiring a learned value function.

## Custom envs
#donkey-generated-track-v0:
donkey-mountain-track-v0:
  # Normalize observations for stable training
  # TODO: try normalizing reward too
  normalize: "{'norm_obs': True, 'norm_reward': False}"
  env_wrapper:
    #- utils.wrappers.AutoencoderCorrectionWrapper
    - utils.wrappers.AutoencoderWrapper
    - utils.wrappers.SpeedWrapper
    #- utils.wrappers.ContinuityCostWrapper:
    #    weight_continuity: 3
    - gymnasium.wrappers.TimeLimit:
        max_episode_steps: 10000
    - utils.wrappers.HistoryWrapper:
        horizon: 2
  callback:
    - utils.callbacks.PLNTensorboardCallback
    - utils.callbacks.LapTimeCallback
    - utils.callbacks.WandbCallback:
        verbose: 2
        model_save_path: "/tmp/stable-baselines/"
        model_save_freq: 100
        gradient_save_freq: 0
  # vec_env_wrapper:
  #   - utils.wrappers.VecForceResetWrapper
  n_timesteps: !!float 2e6
  #policy: 'CnnPolicy'
  policy: 'MlpPolicy'
  learning_rate: !!float 3e-4
  # Number of steps to collect per environment per update
  n_steps: 2048
  # Minibatch size for optimization
  batch_size: 64
  # Number of epochs when optimizing the surrogate loss
  n_epochs: 10
  # Discount factor
  gamma: 0.99
  # Factor for trade-off of bias vs variance for GAE
  gae_lambda: 0.95
  # GRPO-specific: Number of samples per state for group-based advantage estimation
  # Higher values provide more stable gradients but increase computation
  group_size: 4
  # GRPO-specific: Coefficient for KL divergence penalty
  # Controls how much the policy can deviate from the reference policy
  kl_coef: 0.1
  # Clipping parameter for the policy objective (PPO-style)
  clip_range: 0.2
  # Whether to normalize advantages within groups
  normalize_advantage: True
  # Entropy coefficient for exploration
  ent_coef: 0.0
  # Value function coefficient (set to 0 for pure GRPO, >0 for hybrid approach)
  vf_coef: 0.0
  # Maximum value for gradient clipping
  max_grad_norm: 0.5
  # Use generalized State Dependent Exploration (gSDE) for continuous actions
  use_sde: True
  # Sample a new noise matrix every n steps when using gSDE
  sde_sample_freq: 16
  # Limit the KL divergence between updates for early stopping
  target_kl: 0.01
  policy_kwargs: "dict(log_std_init=-2, net_arch=dict(pi=[256, 256], vf=[256, 256]))"

# Alternative configuration for faster lap times with more aggressive exploration
donkey-mountain-track-v0-aggressive:
  normalize: "{'norm_obs': True, 'norm_reward': False}"
  env_wrapper:
    - utils.wrappers.AutoencoderWrapper
    - utils.wrappers.SpeedWrapper
    - gymnasium.wrappers.TimeLimit:
        max_episode_steps: 10000
    - utils.wrappers.HistoryWrapper:
        horizon: 2
  callback:
    - utils.callbacks.PLNTensorboardCallback
    - utils.callbacks.LapTimeCallback
    - utils.callbacks.WandbCallback:
        verbose: 2
        model_save_path: "/tmp/stable-baselines/"
        model_save_freq: 100
        gradient_save_freq: 0
  n_timesteps: !!float 3e6
  policy: 'MlpPolicy'
  learning_rate: !!float 5e-4
  n_steps: 4096
  batch_size: 128
  n_epochs: 15
  gamma: 0.995
  gae_lambda: 0.98
  # Larger group size for more stable advantage estimation
  group_size: 8
  # Lower KL coefficient for more aggressive policy updates
  kl_coef: 0.05
  clip_range: 0.3
  normalize_advantage: True
  # Small entropy bonus for exploration
  ent_coef: 0.01
  vf_coef: 0.0
  max_grad_norm: 0.5
  use_sde: True
  sde_sample_freq: 8
  target_kl: 0.02
  # Note: vf network architecture is kept for potential hybrid GRPO approach (when vf_coef > 0)
  policy_kwargs: "dict(log_std_init=-1, net_arch=dict(pi=[512, 256, 128], vf=[512, 256, 128]))"
