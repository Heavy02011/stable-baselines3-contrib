# TQC Hyperparameters for Autonomous Racing (Donkey Car Simulator)
# This configuration is optimized for the donkey-mountain-track-v0 environment
# and can be used as a starting point for similar autonomous racing tasks.

## Custom envs
#donkey-generated-track-v0:
donkey-mountain-track-v0:
  # Normalize observations using Autoencoder preprocessing (+ other observation normalization)
  # TODO: try normalizing reward too
  normalize: "{'norm_obs': True, 'norm_reward': False}"
  env_wrapper:
    #- utils.wrappers.AutoencoderCorrectionWrapper
    - utils.wrappers.AutoencoderWrapper
    - utils.wrappers.SpeedWrapper
    #- utils.wrappers.ContinuityCostWrapper:
    #    weight_continuity: 3
    - gymnasium.wrappers.TimeLimit:
        max_episode_steps: 10000
    - utils.wrappers.HistoryWrapper:
        horizon: 2
  callback:
    - utils.callbacks.ParallelTrainCallback:
        gradient_steps: 200
    - utils.callbacks.PLNTensorboardCallback
    - utils.callbacks.LapTimeCallback
    - utils.callbacks.WandbCallback:
        verbose: 2
        model_save_path: "/tmp/stable-baselines/"
        model_save_freq: 100
        gradient_save_freq: 0
  # vec_env_wrapper:
  #   - utils.wrappers.VecForceResetWrapper
  n_timesteps: !!float 2e6
  #policy: 'CnnPolicy'
  policy: 'MlpPolicy'
  learning_rate: !!float 7.3e-4
  #learning_rate: !!float 3e-4
  buffer_size: 200000
  batch_size: 256
  ent_coef: 'auto'
  gamma: 0.99
  tau: 0.02
  # train_freq: [1, "episode"]
  train_freq: 200
  # gradient_steps: -1
  gradient_steps: 256
  learning_starts: 5000
  use_sde_at_warmup: True
  use_sde: True
  sde_sample_freq: 16
  policy_kwargs: "dict(log_std_init=-3, net_arch=[256, 256], n_critics=2, use_expln=True)"
