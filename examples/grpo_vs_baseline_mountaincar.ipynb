{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO vs GRPO on MountainCarContinuous-v0\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook compares two reinforcement learning algorithms on the **MountainCarContinuous-v0** environment:\n",
    "\n",
    "- **PPO (Proximal Policy Optimization)**: A popular policy gradient method from `stable-baselines3` that uses a value function (critic) and clipped policy gradients for stable training.\n",
    "\n",
    "- **GRPO (Group Relative Policy Optimization)**: An algorithm implemented in this repository (`sb3_contrib`) that uses relative performance within a group instead of a learned value function. GRPO normalizes advantages within groups of samples, providing stable gradient estimates without the need for a critic network.\n",
    "\n",
    "### Environment: MountainCarContinuous-v0\n",
    "The goal is to drive an underpowered car up a steep hill. The car must build momentum by rocking back and forth. The state space includes position and velocity, and the action is a continuous force applied to the car.\n",
    "\n",
    "### Outcome\n",
    "By the end of this notebook, you will see:\n",
    "1. Training curves comparing episodic rewards for PPO and GRPO\n",
    "2. A side-by-side video of both trained agents for direct visual comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# stable-baselines3 imports\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# sb3_contrib imports (GRPO from this repository)\n",
    "from sb3_contrib import GRPO\n",
    "\n",
    "# Video recording\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "# Video composition\n",
    "from moviepy.editor import VideoFileClip, clips_array\n",
    "\n",
    "# For displaying video in notebook\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print versions for reproducibility\n",
    "import stable_baselines3\n",
    "import sb3_contrib\n",
    "\n",
    "print(f\"gymnasium version: {gym.__version__}\")\n",
    "print(f\"stable-baselines3 version: {stable_baselines3.__version__}\")\n",
    "print(f\"sb3_contrib version: {sb3_contrib.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Environment Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "def make_env(seed: Optional[int] = None) -> gym.Env:\n",
    "    \"\"\"\n",
    "    Create a MountainCarContinuous-v0 environment.\n",
    "    \n",
    "    Args:\n",
    "        seed: Optional random seed for reproducibility.\n",
    "        \n",
    "    Returns:\n",
    "        The created gymnasium environment.\n",
    "    \"\"\"\n",
    "    env = gym.make(\"MountainCarContinuous-v0\")\n",
    "    if seed is not None:\n",
    "        env.reset(seed=seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "TOTAL_TIMESTEPS = 300_000  # Total environment steps for training (suitable for a demo run)\n",
    "SEED = 0                   # Random seed for reproducibility\n",
    "N_EVAL_EPISODES = 10       # Number of episodes for final evaluation\n",
    "VIDEO_EPISODES = 1         # Number of episodes to record for video\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(\"examples/models\", exist_ok=True)\n",
    "os.makedirs(\"examples/videos\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Callback for Logging Training Rewards\n",
    "\n",
    "We create a simple callback to track episodic rewards during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardLoggerCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for logging episode rewards during training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, verbose: int = 0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards: List[float] = []\n",
    "        self.episode_lengths: List[int] = []\n",
    "        self.timesteps: List[int] = []\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        # Check if any episode has finished\n",
    "        if self.locals.get(\"infos\"):\n",
    "            for info in self.locals[\"infos\"]:\n",
    "                if \"episode\" in info:\n",
    "                    self.episode_rewards.append(info[\"episode\"][\"r\"])\n",
    "                    self.episode_lengths.append(info[\"episode\"][\"l\"])\n",
    "                    self.timesteps.append(self.num_timesteps)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Training PPO Baseline\n",
    "\n",
    "**PPO (Proximal Policy Optimization)** is a popular on-policy algorithm that:\n",
    "- Uses a value function (critic) to estimate expected returns\n",
    "- Uses clipped policy gradients to prevent large policy updates\n",
    "- Is known for stable training across many environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment with Monitor wrapper to track episode stats\n",
    "ppo_env = Monitor(make_env(SEED))\n",
    "\n",
    "# Create PPO model\n",
    "ppo_model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    ppo_env,\n",
    "    seed=SEED,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Create callback for logging rewards\n",
    "ppo_callback = RewardLoggerCallback()\n",
    "\n",
    "# Train PPO\n",
    "print(\"Training PPO...\")\n",
    "ppo_model.learn(\n",
    "    total_timesteps=TOTAL_TIMESTEPS,\n",
    "    callback=ppo_callback,\n",
    "    progress_bar=True,\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "ppo_model.save(\"examples/models/ppo_mountaincar\")\n",
    "print(\"PPO model saved to examples/models/ppo_mountaincar.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate PPO\n",
    "ppo_eval_env = make_env(SEED + 100)  # Different seed for evaluation\n",
    "ppo_mean_reward, ppo_std_reward = evaluate_policy(\n",
    "    ppo_model, ppo_eval_env, n_eval_episodes=N_EVAL_EPISODES, deterministic=True\n",
    ")\n",
    "print(f\"PPO Evaluation: Mean reward = {ppo_mean_reward:.2f} +/- {ppo_std_reward:.2f}\")\n",
    "ppo_eval_env.close()\n",
    "\n",
    "# Store training rewards for plotting\n",
    "ppo_rewards = np.array(ppo_callback.episode_rewards)\n",
    "ppo_timesteps = np.array(ppo_callback.timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Training GRPO\n",
    "\n",
    "**GRPO (Group Relative Policy Optimization)** is an algorithm that:\n",
    "- Uses relative performance within a group instead of a learned value function\n",
    "- Normalizes advantages within groups of samples for stable gradient estimates\n",
    "- Reduces computational overhead by eliminating the critic network\n",
    "- Uses KL divergence regularization to prevent policy collapse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment with Monitor wrapper to track episode stats\n",
    "grpo_env = Monitor(make_env(SEED))\n",
    "\n",
    "# Create GRPO model\n",
    "grpo_model = GRPO(\n",
    "    \"MlpPolicy\",\n",
    "    grpo_env,\n",
    "    seed=SEED,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Create callback for logging rewards\n",
    "grpo_callback = RewardLoggerCallback()\n",
    "\n",
    "# Train GRPO\n",
    "print(\"Training GRPO...\")\n",
    "grpo_model.learn(\n",
    "    total_timesteps=TOTAL_TIMESTEPS,\n",
    "    callback=grpo_callback,\n",
    "    progress_bar=True,\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "grpo_model.save(\"examples/models/grpo_mountaincar\")\n",
    "print(\"GRPO model saved to examples/models/grpo_mountaincar.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate GRPO\n",
    "grpo_eval_env = make_env(SEED + 100)  # Different seed for evaluation\n",
    "grpo_mean_reward, grpo_std_reward = evaluate_policy(\n",
    "    grpo_model, grpo_eval_env, n_eval_episodes=N_EVAL_EPISODES, deterministic=True\n",
    ")\n",
    "print(f\"GRPO Evaluation: Mean reward = {grpo_mean_reward:.2f} +/- {grpo_std_reward:.2f}\")\n",
    "grpo_eval_env.close()\n",
    "\n",
    "# Store training rewards for plotting\n",
    "grpo_rewards = np.array(grpo_callback.episode_rewards)\n",
    "grpo_timesteps = np.array(grpo_callback.timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Training Curve Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_rewards(rewards: np.ndarray, window: int = 10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply a simple moving average to smooth reward curves.\n",
    "    \n",
    "    Args:\n",
    "        rewards: Array of episode rewards.\n",
    "        window: Window size for moving average.\n",
    "        \n",
    "    Returns:\n",
    "        Smoothed rewards array.\n",
    "    \"\"\"\n",
    "    if len(rewards) < window:\n",
    "        return rewards\n",
    "    return np.convolve(rewards, np.ones(window) / window, mode=\"valid\")\n",
    "\n",
    "\n",
    "# Create figure for comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot PPO training curve\n",
    "if len(ppo_rewards) > 0:\n",
    "    ppo_smoothed = smooth_rewards(ppo_rewards)\n",
    "    ax.plot(\n",
    "        range(len(ppo_smoothed)),\n",
    "        ppo_smoothed,\n",
    "        label=\"PPO\",\n",
    "        color=\"blue\",\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    # Plot raw rewards with transparency\n",
    "    ax.plot(\n",
    "        range(len(ppo_rewards)),\n",
    "        ppo_rewards,\n",
    "        color=\"blue\",\n",
    "        alpha=0.2,\n",
    "    )\n",
    "\n",
    "# Plot GRPO training curve\n",
    "if len(grpo_rewards) > 0:\n",
    "    grpo_smoothed = smooth_rewards(grpo_rewards)\n",
    "    ax.plot(\n",
    "        range(len(grpo_smoothed)),\n",
    "        grpo_smoothed,\n",
    "        label=\"GRPO\",\n",
    "        color=\"red\",\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    # Plot raw rewards with transparency\n",
    "    ax.plot(\n",
    "        range(len(grpo_rewards)),\n",
    "        grpo_rewards,\n",
    "        color=\"red\",\n",
    "        alpha=0.2,\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_ylabel(\"Episodic Reward\")\n",
    "ax.set_title(\"PPO vs GRPO on MountainCarContinuous-v0\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Video Recording and Side-by-Side Visualization\n",
    "\n",
    "Now we record videos of both trained agents and create a side-by-side comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_video(model, model_name: str, video_dir: str) -> str:\n",
    "    \"\"\"\n",
    "    Record a video of the trained agent.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained RL model.\n",
    "        model_name: Name of the model (for file naming).\n",
    "        video_dir: Directory to save the video.\n",
    "        \n",
    "    Returns:\n",
    "        Path to the recorded video file.\n",
    "    \"\"\"\n",
    "    # Create video directory\n",
    "    video_folder = os.path.join(video_dir, model_name)\n",
    "    os.makedirs(video_folder, exist_ok=True)\n",
    "    \n",
    "    # Create environment with video recorder\n",
    "    env = gym.make(\"MountainCarContinuous-v0\", render_mode=\"rgb_array\")\n",
    "    env = RecordVideo(\n",
    "        env,\n",
    "        video_folder=video_folder,\n",
    "        episode_trigger=lambda x: True,  # Record all episodes\n",
    "        name_prefix=model_name,\n",
    "    )\n",
    "    \n",
    "    # Run episodes\n",
    "    for episode in range(VIDEO_EPISODES):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    # Find the generated video file\n",
    "    video_files = [f for f in os.listdir(video_folder) if f.endswith(\".mp4\")]\n",
    "    if video_files:\n",
    "        return os.path.join(video_folder, video_files[0])\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record videos for both models\n",
    "print(\"Recording PPO video...\")\n",
    "ppo_video_path = record_video(ppo_model, \"ppo_mountaincar\", \"examples/videos\")\n",
    "print(f\"PPO video saved to: {ppo_video_path}\")\n",
    "\n",
    "print(\"\\nRecording GRPO video...\")\n",
    "grpo_video_path = record_video(grpo_model, \"grpo_mountaincar\", \"examples/videos\")\n",
    "print(f\"GRPO video saved to: {grpo_video_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Side-by-Side Video Composition\n",
    "\n",
    "Using moviepy, we create a side-by-side video comparison:\n",
    "- **Left**: PPO agent\n",
    "- **Right**: GRPO agent\n",
    "\n",
    "This allows for direct visual comparison of how each algorithm solves the MountainCar task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the video clips\n",
    "ppo_clip = VideoFileClip(ppo_video_path)\n",
    "grpo_clip = VideoFileClip(grpo_video_path)\n",
    "\n",
    "# Resize to same height if needed\n",
    "target_height = min(ppo_clip.h, grpo_clip.h)\n",
    "ppo_clip_resized = ppo_clip.resize(height=target_height)\n",
    "grpo_clip_resized = grpo_clip.resize(height=target_height)\n",
    "\n",
    "# Make clips the same duration (use the shorter one)\n",
    "min_duration = min(ppo_clip_resized.duration, grpo_clip_resized.duration)\n",
    "ppo_clip_resized = ppo_clip_resized.subclip(0, min_duration)\n",
    "grpo_clip_resized = grpo_clip_resized.subclip(0, min_duration)\n",
    "\n",
    "# Create side-by-side composition\n",
    "side_by_side = clips_array([[ppo_clip_resized, grpo_clip_resized]])\n",
    "\n",
    "# Save the composite video\n",
    "output_path = \"examples/videos/ppo_vs_grpo_mountaincar_side_by_side.mp4\"\n",
    "side_by_side.write_videofile(output_path, fps=30, codec=\"libx264\")\n",
    "\n",
    "# Close clips\n",
    "ppo_clip.close()\n",
    "grpo_clip.close()\n",
    "\n",
    "print(f\"\\nSide-by-side video saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Display Side-by-Side Video\n",
    "\n",
    "The video below shows:\n",
    "- **Left**: PPO agent solving MountainCarContinuous-v0\n",
    "- **Right**: GRPO agent solving MountainCarContinuous-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the side-by-side video in the notebook\n",
    "video_html = f\"\"\"\n",
    "<video width=\"800\" controls>\n",
    "    <source src=\"{output_path}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "HTML(video_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated a comparison between PPO and GRPO on the MountainCarContinuous-v0 environment.\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "| Aspect | PPO | GRPO |\n",
    "|--------|-----|------|\n",
    "| Value Function | Uses a learned critic | No critic needed |\n",
    "| Advantage Estimation | GAE with value function | Group-relative normalization |\n",
    "| Computational Cost | Higher (two networks) | Lower (single network) |\n",
    "| Regularization | Clipped policy gradients | KL divergence + clipping |\n",
    "\n",
    "### Files Generated:\n",
    "- `examples/models/ppo_mountaincar.zip` - Trained PPO model\n",
    "- `examples/models/grpo_mountaincar.zip` - Trained GRPO model\n",
    "- `examples/videos/ppo_mountaincar/` - PPO agent video\n",
    "- `examples/videos/grpo_mountaincar/` - GRPO agent video\n",
    "- `examples/videos/ppo_vs_grpo_mountaincar_side_by_side.mp4` - Side-by-side comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: Close training environments\n",
    "ppo_env.close()\n",
    "grpo_env.close()\n",
    "\n",
    "print(\"All environments closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
