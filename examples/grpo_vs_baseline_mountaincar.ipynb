{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO vs GRPO on MountainCarContinuous-v0\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook compares two reinforcement learning algorithms on the **MountainCarContinuous-v0** environment:\n",
    "\n",
    "- **PPO (Proximal Policy Optimization)**: A popular policy gradient method from `stable-baselines3` that uses a value function (critic) and clipped policy gradients for stable training.\n",
    "\n",
    "- **GRPO (Group Relative Policy Optimization)**: An algorithm implemented in this repository (`sb3_contrib`) that uses relative performance within a group instead of a learned value function. GRPO normalizes advantages within groups of samples, providing stable gradient estimates without the need for a critic network.\n",
    "\n",
    "### Environment: MountainCarContinuous-v0\n",
    "The goal is to drive an underpowered car up a steep hill. The car must build momentum by rocking back and forth. The state space includes position and velocity, and the action is a continuous force applied to the car.\n",
    "\n",
    "### Outcome\n",
    "By the end of this notebook, you will see:\n",
    "1. Training curves comparing episodic rewards for PPO and GRPO\n",
    "2. A side-by-side video of both trained agents for direct visual comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-05 14:21:22.010669: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-05 14:21:22.049756: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-05 14:21:22.987992: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-05 14:21:22.987992: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# stable-baselines3 imports\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# sb3_contrib imports (GRPO from this repository)\n",
    "from sb3_contrib import GRPO\n",
    "\n",
    "# TensorBoard logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Video recording\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "# Video composition (moviepy 2.x uses moviepy.video.io instead of moviepy.editor)\n",
    "try:\n",
    "    from moviepy.editor import VideoFileClip, clips_array\n",
    "except ImportError:\n",
    "    # For moviepy 2.x\n",
    "    from moviepy import VideoFileClip, concatenate_videoclips\n",
    "\n",
    "# For displaying video in notebook\n",
    "from IPython.display import HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gymnasium version: 1.2.2\n",
      "stable-baselines3 version: 2.7.0\n",
      "sb3_contrib version: 2.7.1a3\n"
     ]
    }
   ],
   "source": [
    "# Print versions for reproducibility\n",
    "import stable_baselines3\n",
    "import sb3_contrib\n",
    "\n",
    "print(f\"gymnasium version: {gym.__version__}\")\n",
    "print(f\"stable-baselines3 version: {stable_baselines3.__version__}\")\n",
    "print(f\"sb3_contrib version: {sb3_contrib.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Environment Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "def make_env(seed: Optional[int] = None) -> gym.Env:\n",
    "    \"\"\"\n",
    "    Create a MountainCarContinuous-v0 environment.\n",
    "    \n",
    "    Args:\n",
    "        seed: Optional random seed for reproducibility.\n",
    "        \n",
    "    Returns:\n",
    "        The created gymnasium environment.\n",
    "    \"\"\"\n",
    "    env = gym.make(\"MountainCarContinuous-v0\")\n",
    "    if seed is not None:\n",
    "        env.reset(seed=seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2,000,000 timesteps (~10000 episodes)\n",
      "PPO params: lr=0.0007, n_steps=8, batch=256, gamma=0.9999\n",
      "GRPO params: lr=0.0007, n_steps=8, batch=256, gamma=0.9999\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "# MountainCarContinuous-v0 requires extensive training\n",
    "TOTAL_TIMESTEPS = 2_000_000  # Increased to 2M - this environment is very difficult\n",
    "SEED = 0                      # Random seed for reproducibility\n",
    "N_EVAL_EPISODES = 10          # Number of episodes for final evaluation\n",
    "VIDEO_EPISODES = 1            # Number of episodes to record for video\n",
    "\n",
    "# Hyperparameters from RL Baselines3 Zoo (proven to work)\n",
    "# Source: https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/ppo.yml\n",
    "PPO_PARAMS = {\n",
    "    'learning_rate': 7e-4,      # Lower LR for more stable learning (was 1e-3)\n",
    "    'n_steps': 8,               # Much smaller for faster value updates (was 16)\n",
    "    'batch_size': 256,          # Larger batches for stable gradients (was 16)\n",
    "    'n_epochs': 10,             # Keep default\n",
    "    'gamma': 0.9999,            # Very high discount - long-term planning crucial\n",
    "    'gae_lambda': 0.94,         # Lower GAE for less bias (was 0.98)\n",
    "    'clip_range': 0.2,          # Default clip range\n",
    "    'ent_coef': 0.0,            # No entropy bonus\n",
    "    'vf_coef': 0.5,             # Value function coefficient\n",
    "    'max_grad_norm': 0.5,       # Gradient clipping\n",
    "}\n",
    "\n",
    "GRPO_PARAMS = {\n",
    "    'learning_rate': 7e-4,      # Match PPO (was 1e-3)\n",
    "    'n_steps': 8,               # Match PPO (was 16)\n",
    "    'batch_size': 256,          # Match PPO (was 16)\n",
    "    'gamma': 0.9999,            # Match PPO\n",
    "    'group_size': 32,           # Larger groups for better statistics (was 8)\n",
    "    'kl_coef': 0.05,            # Stronger KL penalty (was 0.02)\n",
    "    'clip_range': 0.2,          # Match PPO\n",
    "    'max_grad_norm': 0.5,       # Gradient clipping\n",
    "}\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs('examples/models', exist_ok=True)\n",
    "os.makedirs('examples/videos', exist_ok=True)\n",
    "os.makedirs('/tmp/tensorboard_logs', exist_ok=True)\n",
    "\n",
    "print(f'Training for {TOTAL_TIMESTEPS:,} timesteps (~{TOTAL_TIMESTEPS // 200} episodes)')\n",
    "print(f'PPO params: lr={PPO_PARAMS[\"learning_rate\"]}, n_steps={PPO_PARAMS[\"n_steps\"]}, batch={PPO_PARAMS[\"batch_size\"]}, gamma={PPO_PARAMS[\"gamma\"]}')\n",
    "print(f'GRPO params: lr={GRPO_PARAMS[\"learning_rate\"]}, n_steps={GRPO_PARAMS[\"n_steps\"]}, batch={GRPO_PARAMS[\"batch_size\"]}, gamma={GRPO_PARAMS[\"gamma\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Callback for Logging Training Rewards\n",
    "\n",
    "We create a simple callback to track episodic rewards during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardLoggerCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for logging episode rewards during training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, verbose: int = 0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards: List[float] = []\n",
    "        self.episode_lengths: List[int] = []\n",
    "        self.timesteps: List[int] = []\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        # Check if any episode has finished\n",
    "        if self.locals.get(\"infos\"):\n",
    "            for info in self.locals[\"infos\"]:\n",
    "                if \"episode\" in info:\n",
    "                    self.episode_rewards.append(info[\"episode\"][\"r\"])\n",
    "                    self.episode_lengths.append(info[\"episode\"][\"l\"])\n",
    "                    self.timesteps.append(self.num_timesteps)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Training PPO Baseline\n",
    "\n",
    "**PPO (Proximal Policy Optimization)** is a popular on-policy algorithm that:\n",
    "- Uses a value function (critic) to estimate expected returns\n",
    "- Uses clipped policy gradients to prevent large policy updates\n",
    "- Is known for stable training across many environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rainer/anaconda3/envs/rlracing/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:155: UserWarning: You have specified a mini-batch size of 256, but because the `RolloutBuffer` is of size `n_steps * n_envs = 8`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 8\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=8 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ad5a2677a9465d811d90ae8727cd85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rainer/anaconda3/envs/rlracing/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO config: lr=0.0007, n_steps=8, batch=256, gamma=0.9999\n",
      "\n",
      "Training PPO...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO model saved to examples/models/ppo_mountaincar.zip\n"
     ]
    }
   ],
   "source": [
    "# Create environment with Monitor wrapper to track episode stats\n",
    "ppo_env = Monitor(make_env(SEED))\n",
    "\n",
    "# Create PPO model with tuned hyperparameters\n",
    "ppo_model = PPO(\n",
    "    'MlpPolicy',\n",
    "    ppo_env,\n",
    "    seed=SEED,\n",
    "    verbose=0,\n",
    "    tensorboard_log='/tmp/tensorboard_logs',\n",
    "    **PPO_PARAMS,  # Apply tuned hyperparameters\n",
    ")\n",
    "\n",
    "print(f'PPO config: lr={ppo_model.learning_rate}, n_steps={ppo_model.n_steps}, batch={ppo_model.batch_size}, gamma={ppo_model.gamma}')\n",
    "\n",
    "# Create callback for logging rewards\n",
    "ppo_callback = RewardLoggerCallback()\n",
    "\n",
    "# Train PPO\n",
    "print('\\nTraining PPO...')\n",
    "ppo_model.learn(\n",
    "    total_timesteps=TOTAL_TIMESTEPS,\n",
    "    callback=ppo_callback,\n",
    "    progress_bar=True,\n",
    "    tb_log_name='PPO',  # This creates PPO_1, PPO_2, etc.\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "ppo_model.save('examples/models/ppo_mountaincar')\n",
    "print('PPO model saved to examples/models/ppo_mountaincar.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rainer/anaconda3/envs/rlracing/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:70: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO Evaluation: Mean reward = -99.90 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "# Evaluate PPO\n",
    "ppo_eval_env = make_env(SEED + 100)  # Different seed for evaluation\n",
    "ppo_mean_reward, ppo_std_reward = evaluate_policy(\n",
    "    ppo_model, ppo_eval_env, n_eval_episodes=N_EVAL_EPISODES, deterministic=True\n",
    ")\n",
    "print(f\"PPO Evaluation: Mean reward = {ppo_mean_reward:.2f} +/- {ppo_std_reward:.2f}\")\n",
    "ppo_eval_env.close()\n",
    "\n",
    "# Store training rewards for plotting\n",
    "ppo_rewards = np.array(ppo_callback.episode_rewards)\n",
    "ppo_timesteps = np.array(ppo_callback.timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Training GRPO\n",
    "\n",
    "**GRPO (Group Relative Policy Optimization)** is an algorithm that:\n",
    "- Uses relative performance within a group instead of a learned value function\n",
    "- Normalizes advantages within groups of samples for stable gradient estimates\n",
    "- Reduces computational overhead by eliminating the critic network\n",
    "- Uses KL divergence regularization to prevent policy collapse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119097832ffb4818873de2caf4ec78e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRPO config: lr=0.0007, n_steps=8, batch=256, gamma=0.9999\n",
      "\n",
      "Training GRPO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rainer/pr/github/stable-baselines3-contrib/sb3_contrib/grpo/grpo.py:161: UserWarning: You have specified a mini-batch size of 256, but because the `RolloutBuffer` is of size `n_steps * n_envs = 8`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 8\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=8 and n_envs=1)\n",
      "  warnings.warn(\n",
      "/home/rainer/anaconda3/envs/rlracing/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run GRPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Train GRPO\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining GRPO...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mgrpo_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTOTAL_TIMESTEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrpo_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mGRPO\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# This creates GRPO_1, GRPO_2, etc.\u001b[39;49;00m\n\u001b[32m     26\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[32m     29\u001b[39m grpo_model.save(\u001b[33m'\u001b[39m\u001b[33mexamples/models/grpo_mountaincar\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pr/github/stable-baselines3-contrib/sb3_contrib/grpo/grpo.py:383\u001b[39m, in \u001b[36mGRPO.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    374\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    375\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfGRPO,\n\u001b[32m    376\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    381\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    382\u001b[39m ) -> SelfGRPO:\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/rlracing/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:337\u001b[39m, in \u001b[36mOnPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    334\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    335\u001b[39m         \u001b[38;5;28mself\u001b[39m.dump_logs(iteration)\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m callback.on_training_end()\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pr/github/stable-baselines3-contrib/sb3_contrib/grpo/grpo.py:342\u001b[39m, in \u001b[36mGRPO.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;66;03m# Optimization step\u001b[39;00m\n\u001b[32m    341\u001b[39m \u001b[38;5;28mself\u001b[39m.policy.optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[38;5;66;03m# Clip grad norm\u001b[39;00m\n\u001b[32m    344\u001b[39m th.nn.utils.clip_grad_norm_(\u001b[38;5;28mself\u001b[39m.policy.parameters(), \u001b[38;5;28mself\u001b[39m.max_grad_norm)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/rlracing/lib/python3.11/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/rlracing/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/rlracing/lib/python3.11/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Create environment with Monitor wrapper to track episode stats\n",
    "grpo_env = Monitor(make_env(SEED))\n",
    "\n",
    "# Create GRPO model with tuned hyperparameters\n",
    "grpo_model = GRPO(\n",
    "    'MlpPolicy',\n",
    "    grpo_env,\n",
    "    seed=SEED,\n",
    "    verbose=0,\n",
    "    tensorboard_log='/tmp/tensorboard_logs',\n",
    "    **GRPO_PARAMS,  # Apply tuned hyperparameters\n",
    ")\n",
    "\n",
    "print(f'GRPO config: lr={grpo_model.learning_rate}, n_steps={grpo_model.n_steps}, batch={grpo_model.batch_size}, gamma={grpo_model.gamma}')\n",
    "\n",
    "# Create callback for logging rewards\n",
    "grpo_callback = RewardLoggerCallback()\n",
    "\n",
    "# Train GRPO\n",
    "print('\\nTraining GRPO...')\n",
    "grpo_model.learn(\n",
    "    total_timesteps=TOTAL_TIMESTEPS,\n",
    "    callback=grpo_callback,\n",
    "    progress_bar=True,\n",
    "    tb_log_name='GRPO',  # This creates GRPO_1, GRPO_2, etc.\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "grpo_model.save('examples/models/grpo_mountaincar')\n",
    "print('GRPO model saved to examples/models/grpo_mountaincar.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate GRPO\n",
    "grpo_eval_env = make_env(SEED + 100)  # Different seed for evaluation\n",
    "grpo_mean_reward, grpo_std_reward = evaluate_policy(\n",
    "    grpo_model, grpo_eval_env, n_eval_episodes=N_EVAL_EPISODES, deterministic=True\n",
    ")\n",
    "print(f\"GRPO Evaluation: Mean reward = {grpo_mean_reward:.2f} +/- {grpo_std_reward:.2f}\")\n",
    "grpo_eval_env.close()\n",
    "\n",
    "# Store training rewards for plotting\n",
    "grpo_rewards = np.array(grpo_callback.episode_rewards)\n",
    "grpo_timesteps = np.array(grpo_callback.timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Training Curve Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_rewards(rewards: np.ndarray, window: int = 10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply a simple moving average to smooth reward curves.\n",
    "    \n",
    "    Args:\n",
    "        rewards: Array of episode rewards.\n",
    "        window: Window size for moving average.\n",
    "        \n",
    "    Returns:\n",
    "        Smoothed rewards array.\n",
    "    \"\"\"\n",
    "    if len(rewards) < window:\n",
    "        return rewards\n",
    "    return np.convolve(rewards, np.ones(window) / window, mode=\"valid\")\n",
    "\n",
    "\n",
    "# Create figure for comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot PPO training curve\n",
    "if len(ppo_rewards) > 0:\n",
    "    ppo_smoothed = smooth_rewards(ppo_rewards)\n",
    "    ax.plot(\n",
    "        range(len(ppo_smoothed)),\n",
    "        ppo_smoothed,\n",
    "        label=\"PPO\",\n",
    "        color=\"blue\",\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    # Plot raw rewards with transparency\n",
    "    ax.plot(\n",
    "        range(len(ppo_rewards)),\n",
    "        ppo_rewards,\n",
    "        color=\"blue\",\n",
    "        alpha=0.2,\n",
    "    )\n",
    "\n",
    "# Plot GRPO training curve\n",
    "if len(grpo_rewards) > 0:\n",
    "    grpo_smoothed = smooth_rewards(grpo_rewards)\n",
    "    ax.plot(\n",
    "        range(len(grpo_smoothed)),\n",
    "        grpo_smoothed,\n",
    "        label=\"GRPO\",\n",
    "        color=\"red\",\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    # Plot raw rewards with transparency\n",
    "    ax.plot(\n",
    "        range(len(grpo_rewards)),\n",
    "        grpo_rewards,\n",
    "        color=\"red\",\n",
    "        alpha=0.2,\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_ylabel(\"Episodic Reward\")\n",
    "ax.set_title(\"PPO vs GRPO on MountainCarContinuous-v0\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Video Recording and Side-by-Side Visualization\n",
    "\n",
    "Now we record videos of both trained agents and create a side-by-side comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_video(model, model_name: str, video_dir: str) -> str:\n",
    "    \"\"\"\n",
    "    Record a video of the trained agent.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained RL model.\n",
    "        model_name: Name of the model (for file naming).\n",
    "        video_dir: Directory to save the video.\n",
    "        \n",
    "    Returns:\n",
    "        Path to the recorded video file.\n",
    "    \"\"\"\n",
    "    # Create video directory\n",
    "    video_folder = os.path.join(video_dir, model_name)\n",
    "    os.makedirs(video_folder, exist_ok=True)\n",
    "    \n",
    "    # Create environment with video recorder\n",
    "    env = gym.make(\"MountainCarContinuous-v0\", render_mode=\"rgb_array\")\n",
    "    env = RecordVideo(\n",
    "        env,\n",
    "        video_folder=video_folder,\n",
    "        episode_trigger=lambda x: True,  # Record all episodes\n",
    "        name_prefix=model_name,\n",
    "    )\n",
    "    \n",
    "    # Run episodes\n",
    "    for episode in range(VIDEO_EPISODES):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    # Find the generated video file\n",
    "    video_files = [f for f in os.listdir(video_folder) if f.endswith(\".mp4\")]\n",
    "    if video_files:\n",
    "        return os.path.join(video_folder, video_files[0])\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record videos for both models\n",
    "print(\"Recording PPO video...\")\n",
    "ppo_video_path = record_video(ppo_model, \"ppo_mountaincar\", \"examples/videos\")\n",
    "print(f\"PPO video saved to: {ppo_video_path}\")\n",
    "\n",
    "print(\"\\nRecording GRPO video...\")\n",
    "grpo_video_path = record_video(grpo_model, \"grpo_mountaincar\", \"examples/videos\")\n",
    "print(f\"GRPO video saved to: {grpo_video_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Side-by-Side Video Composition\n",
    "\n",
    "Using moviepy, we create a side-by-side video comparison:\n",
    "- **Left**: PPO agent\n",
    "- **Right**: GRPO agent\n",
    "\n",
    "This allows for direct visual comparison of how each algorithm solves the MountainCar task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the video clips\n",
    "ppo_clip = VideoFileClip(ppo_video_path)\n",
    "grpo_clip = VideoFileClip(grpo_video_path)\n",
    "\n",
    "# Resize to same height if needed (moviepy 2.x uses resized method)\n",
    "target_height = min(ppo_clip.h, grpo_clip.h)\n",
    "try:\n",
    "    # moviepy 1.x\n",
    "    ppo_clip_resized = ppo_clip.resize(height=target_height)\n",
    "    grpo_clip_resized = grpo_clip.resize(height=target_height)\n",
    "except AttributeError:\n",
    "    # moviepy 2.x\n",
    "    scale_ppo = target_height / ppo_clip.h\n",
    "    scale_grpo = target_height / grpo_clip.h\n",
    "    ppo_clip_resized = ppo_clip.resized(scale_ppo)\n",
    "    grpo_clip_resized = grpo_clip.resized(scale_grpo)\n",
    "\n",
    "# Make clips the same duration (use the shorter one)\n",
    "min_duration = min(ppo_clip_resized.duration, grpo_clip_resized.duration)\n",
    "try:\n",
    "    # moviepy 1.x\n",
    "    ppo_clip_resized = ppo_clip_resized.subclip(0, min_duration)\n",
    "    grpo_clip_resized = grpo_clip_resized.subclip(0, min_duration)\n",
    "except AttributeError:\n",
    "    # moviepy 2.x\n",
    "    ppo_clip_resized = ppo_clip_resized.subclipped(0, min_duration)\n",
    "    grpo_clip_resized = grpo_clip_resized.subclipped(0, min_duration)\n",
    "\n",
    "# Create side-by-side composition\n",
    "try:\n",
    "    # Try moviepy 1.x style\n",
    "    from moviepy.editor import clips_array\n",
    "    side_by_side = clips_array([[ppo_clip_resized, grpo_clip_resized]])\n",
    "except (ImportError, AttributeError):\n",
    "    # For moviepy 2.x - create side by side manually\n",
    "    from moviepy import CompositeVideoClip\n",
    "    \n",
    "    # Calculate total width\n",
    "    total_width = ppo_clip_resized.w + grpo_clip_resized.w\n",
    "    \n",
    "    # Position clips side by side\n",
    "    side_by_side = CompositeVideoClip(\n",
    "        [\n",
    "            ppo_clip_resized.with_position((0, 0)),\n",
    "            grpo_clip_resized.with_position((ppo_clip_resized.w, 0))\n",
    "        ],\n",
    "        size=(total_width, target_height)\n",
    "    )\n",
    "\n",
    "# Save the composite video\n",
    "output_path = \"examples/videos/ppo_vs_grpo_mountaincar_side_by_side.mp4\"\n",
    "side_by_side.write_videofile(output_path, fps=30, codec=\"libx264\")\n",
    "\n",
    "# Close clips\n",
    "ppo_clip.close()\n",
    "grpo_clip.close()\n",
    "\n",
    "print(f\"\\nSide-by-side video saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Display Side-by-Side Video\n",
    "\n",
    "The video below shows:\n",
    "- **Left**: PPO agent solving MountainCarContinuous-v0\n",
    "- **Right**: GRPO agent solving MountainCarContinuous-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the side-by-side video in the notebook\n",
    "video_html = f\"\"\"\n",
    "<video width=\"800\" controls>\n",
    "    <source src=\"{output_path}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "HTML(video_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11) View Results in TensorBoard\n",
    "\n",
    "TensorBoard logs have been saved during training in `/tmp/tensorboard_logs/`.\n",
    "\n",
    "### Option 1: Launch TensorBoard from terminal\n",
    "```bash\n",
    "tensorboard --logdir /tmp/tensorboard_logs\n",
    "```\n",
    "Then open your browser to http://localhost:6006\n",
    "\n",
    "### Option 2: Launch TensorBoard in Jupyter (next cell)\n",
    "\n",
    "### Run Organization:\n",
    "- **PPO_1, PPO_2, ...**: PPO training runs\n",
    "- **GRPO_1, GRPO_2, ...**: GRPO training runs\n",
    "\n",
    "Each time you train, a new numbered run is created (e.g., PPO_1, PPO_2) so you can compare multiple training runs.\n",
    "\n",
    "### Metrics Available:\n",
    "- **rollout/ep_rew_mean**: Mean episode reward (should reach 90-100 for success)\n",
    "- **rollout/ep_len_mean**: Mean episode length (should decrease from ~1000 to ~100-300)\n",
    "- **train/learning_rate**: Learning rate over time\n",
    "- **train/loss**: Policy and value losses\n",
    "- **train/policy_gradient_loss**: Policy gradient loss\n",
    "- **train/value_loss**: Value function loss (PPO only)\n",
    "\n",
    "### Comparing PPO vs GRPO:\n",
    "In TensorBoard, you can overlay the PPO and GRPO runs to directly compare:\n",
    "- Training efficiency (how quickly rewards improve)\n",
    "- Final performance (maximum achieved reward)\n",
    "- Stability (variance in rewards over time)\n",
    "\n",
    "**Tip**: Use the TensorBoard UI to select which runs to display (e.g., PPO_1 vs GRPO_1) for side-by-side comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch TensorBoard in notebook (optional)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /tmp/tensorboard_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated a comparison between PPO and GRPO on the MountainCarContinuous-v0 environment.\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "| Aspect | PPO | GRPO |\n",
    "|--------|-----|------|\n",
    "| Value Function | Uses a learned critic | No critic needed |\n",
    "| Advantage Estimation | GAE with value function | Group-relative normalization |\n",
    "| Computational Cost | Higher (two networks) | Lower (single network) |\n",
    "| Regularization | Clipped policy gradients | KL divergence + clipping |\n",
    "\n",
    "### Files Generated:\n",
    "- `examples/models/ppo_mountaincar.zip` - Trained PPO model\n",
    "- `examples/models/grpo_mountaincar.zip` - Trained GRPO model\n",
    "- `examples/videos/ppo_mountaincar/` - PPO agent video\n",
    "- `examples/videos/grpo_mountaincar/` - GRPO agent video\n",
    "- `examples/videos/ppo_vs_grpo_mountaincar_side_by_side.mp4` - Side-by-side comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: Close training environments\n",
    "ppo_env.close()\n",
    "grpo_env.close()\n",
    "\n",
    "print(\"All environments closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(rlracing) rainer@borg-mini:~/pr/github/stable-baselines3-contrib$ python -m rl_zoo3.train --algo ppo --env MountainCar-v0 -n 50000 -optimize --n-trials 1000 --n-jobs 2 --sampler random --pruner median\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlracing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
